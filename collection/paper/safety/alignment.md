# A2. Alignment
- [2025/09] **[Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing Alignment](https://arxiv.org/abs/2509.22745)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![MoE](https://img.shields.io/badge/MoE-87b800)
- [2025/09] **[GSPR: Aligning LLM Safeguards as Generalizable Safety Policy Reasoners](https://arxiv.org/abs/2509.24418)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **[Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'25_(Finding)](https://img.shields.io/badge/EMNLP'25_(Finding)-f1b800)
- [2025/08] **[PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality](https://arxiv.org/abs/2508.18649)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/08] **[Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149?)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[SDD: Self-Degraded Defense against Malicious Fine-tuning](https://arxiv.org/abs/2507.21182)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/07] **[Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment](https://arxiv.org/abs/2507.18631)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868?)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/07] **[ARMOR: Aligning Secure and Safe Large Language Models via Meticulous Reasoning](https://arxiv.org/abs/2507.11500)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[TuneShield: Mitigating Toxicity in Conversational AI while Fine-tuning on Untrusted Data](https://arxiv.org/abs/2507.05660)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/07] **[Emergent misalignment as prompt sensitivity: A research note](https://arxiv.org/abs/2507.06253)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing](https://arxiv.org/abs/2507.07056)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/07] **[On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Improving Large Language Model Safety with Contrastive Representation Learning](https://arxiv.org/abs/2506.11938)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/zhangyitonggg/DAVSP) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/06] **[Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning](https://arxiv.org/abs/2506.03850)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'25](https://img.shields.io/badge/ICML'25-f1b800)
- [2025/06] **[Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Accidental Vulnerability: Factors in Fine-Tuning that Shift Model Safeguards](https://arxiv.org/abs/2505.16789?)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[MTSA: Multi-turn Safety Alignment for LLMs through Multi-round Red-teaming](https://arxiv.org/abs/2505.17147)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets](https://arxiv.org/abs/2505.12038)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'25](https://img.shields.io/badge/ICML'25-f1b800)
- [2025/05] **[One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Alleviating the Fear of Losing Alignment in LLM Fine-tuning](https://arxiv.org/abs/2504.09757)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[The H-Elena Trojan Virus to Infect Model Weights: A Wake-Up Call on the Security Risks of Malicious Fine-Tuning](https://arxiv.org/abs/2504.03823)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Safety is Not Only About Refusal: Reasoning-Enhanced Fine-tuning for Interpretable LLM Safety](https://arxiv.org/abs/2503.05021)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Safety Tax: Safety Alignment Makes Your Large Reasoning Models Less Reasonable](https://arxiv.org/abs/2503.00555)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/abs/2503.03710)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment](https://arxiv.org/abs/2502.11244)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data](https://arxiv.org/abs/2502.19537)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment](https://arxiv.org/abs/2502.15334)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Fundamental Limitations in Defending LLM Finetuning APIs](https://arxiv.org/abs/2502.14828)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[LLM Safety for Children](https://arxiv.org/abs/2502.12552v1)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/02] **[VLM-Guard: Safeguarding Vision-Language Models via Fulfilling Safety Alignment Gap](https://arxiv.org/abs/2502.10486)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/02] **[Safety Misalignment Against Large Language Models](https://www.ndss-symposium.org/ndss-paper/safety-misalignment-against-large-language-models/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/ThuCCSLab/misalignment) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2025/02] **[The dark deep side of DeepSeek: Fine-tuning attacks against the safety alignment of CoT-enabled models](https://arxiv.org/abs/2502.01225)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/01] **[Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation ](https://arxiv.org/abs/2501.17433)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/01] **[Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/12] **[Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs](https://arxiv.org/abs/2412.06843)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'25](https://img.shields.io/badge/ICLR'25-f1b800)
- [2024/12] **[On Evaluating the Durability of Safeguards for Open-Weight LLMs](https://arxiv.org/abs/2412.07097)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://arxiv.org/pdf/2410.06625)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/DripNowhy/ETA) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/10] **[Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attack](https://arxiv.org/abs/2410.18210)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[On the Role of Attention Heads in Large Language Model Safety](https://arxiv.org/abs/2410.13708)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Superficial Safety Alignment Hypothesis](https://arxiv.org/abs/2410.10862)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks](https://arxiv.org/abs/2410.03769)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation](https://arxiv.org/abs/2409.01586)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/Booster) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey](https://arxiv.org/abs/2409.18169)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/08] **[Safety Layers of Aligned Large Language Models: The Key to LLM Security](https://arxiv.org/abs/2408.17003)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2408.09600)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Can Editing LLMs Inject Harm?](https://arxiv.org/abs/2407.20224)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://llm-editing.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[The Better Angels of Machine Personality: How Personality Relates to LLM Safety](https://arxiv.org/abs/2407.12344)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Safety Arithmetic: A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations](https://arxiv.org/abs/2406.11801)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/declare-lab/safety-arithmetic) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'24](https://img.shields.io/badge/EMNLP'24-f1b800)
- [2024/06] **[SafeInfer: Context Adaptive Decoding Time Safety Alignment for Large Language Models](https://arxiv.org/abs/2406.12274)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AAAI'25](https://img.shields.io/badge/AAAI'25-f1b800)
- [2024/06] **[MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?](https://arxiv.org/abs/2406.17806)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/06] **[Cross-Modality Safety Alignment](https://arxiv.org/abs/2406.15279)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/06] **[SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model](https://arxiv.org/abs/2406.12030)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/06] **[Model Merging and Safety Alignment: One Bad Model Spoils the Bunch](https://arxiv.org/abs/2406.14563)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[ChatBug: A Common Vulnerability of Aligned LLMs Induced by Chat Templates](https://arxiv.org/abs/2406.12935)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/uw-nsl/ChatBug) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AAAI'25](https://img.shields.io/badge/AAAI'25-f1b800)
- [2024/06] **[Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models](https://arxiv.org/abs/2406.10630)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Safety Alignment Should Be Made More Than Just a Few Tokens Deep ](https://arxiv.org/abs/2406.05946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Decoupled Alignment for Robust Plug-and-Play Adaptation](https://arxiv.org/abs/2406.01514)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2405.18641)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/Lisa) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2024/05] **[MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability](https://arxiv.org/abs/2405.14488)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/DYR1/MoGU) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Safety Alignment for Vision Language Models](https://www.arxiv.org/abs/2405.13581)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/05] **[Learning diverse attacks on large language models for robust red-teaming and safety tuning](https://arxiv.org/abs/2405.18540)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[A safety realignment framework via subspace-oriented model fusion for large language models](https://arxiv.org/pdf/2405.09055)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[A Causal Explainable Guardrails for Large Language Models ](https://arxiv.org/abs/2405.04160)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness](https://arxiv.org/abs/2404.18870)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models](https://arxiv.org/abs/2403.11838)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NAACL'24](https://img.shields.io/badge/NAACL'24-f1b800)
- [2024/03] **[Using Hallucinations to Bypass RLHF Filters](https://arxiv.org/abs/2403.04769)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Attack](https://img.shields.io/badge/Attack-87b800)
- [2024/03] **[Aligners: Decoupling LLMs and Alignment](https://arxiv.org/abs/2403.04224)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Tiny)](https://img.shields.io/badge/ICLR'24_(Tiny)-f1b800)
- [2024/03] **[Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization](https://arxiv.org/abs/2403.03419)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning](https://arxiv.org/abs/2402.01109)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/git-disl/Vaccine) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'24](https://img.shields.io/badge/NeurIPS'24-f1b800)
- [2024/02] **[Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/abs/2402.18540)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!](https://arxiv.org/abs/2402.12343)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic](https://arxiv.org/abs/2402.11746)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Learning to Edit: Aligning LLMs with Knowledge Editing](https://arxiv.org/abs/2402.11905)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[DeAL: Decoding-time Alignment for Large Language Models](https://arxiv.org/abs/2402.06147)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications](https://arxiv.org/abs/2402.05162)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://boyiwei.com/alignment-attribution/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Agent Alignment in Evolving Social Norms](https://arxiv.org/abs/2401.04620)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2023/12] **[Alignment for Honesty](https://arxiv.org/abs/2312.07000)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Exploiting Novel GPT-4 APIs](https://arxiv.org/abs/2312.14302)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Removing RLHF Protections in GPT-4 via Fine-Tuning](https://arxiv.org/abs/2311.05553)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[AI Alignment: A Comprehensive Survey](https://arxiv.org/abs/2310.19852)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/PKU-Alignment/AlignmentSurvey?tab=readme-ov-file) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/10] **[Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models](https://arxiv.org/abs/2310.02949v1)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/BeyonderXX/ShadowAlignment) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/agi-templar/Stable-Alignment) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Alignment as Reward-Guided Search](https://openreview.net/forum?id=shgx0eqdw6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment](https://openreview.net/forum?id=LNLjU5C5dK)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints](https://openreview.net/forum?id=2cRzmWXK9N)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[CAS: A Probability-Based Approach for Universal Condition Alignment Score](https://openreview.net/forum?id=E78OaH2s3f)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[CPPO: Continual Learning for Reinforcement Learning with Human Feedback](https://openreview.net/forum?id=86zAUE80pP)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!](https://openreview.net/forum?id=hTEGyKf0dZ)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Oral)](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
- [2023/09] **[FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets](https://openreview.net/forum?id=CYmF38ysDa)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis](https://openreview.net/forum?id=aA33A70IO6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Generative Judge for Evaluating Alignment](https://openreview.net/forum?id=gtkFw6sZGS)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Group Preference Optimization: Few-Shot Alignment of Large Language Models](https://openreview.net/forum?id=DpFeMH4l8Q)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Improving Generalization of Alignment with Human Preferences through Group Invariant Learning](https://openreview.net/forum?id=fwCoLe3TAX)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Large Language Models as Automated Aligners for benchmarking Vision-Language Models](https://openreview.net/forum?id=kZEXgtMNNo)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models](https://openreview.net/forum?id=dKl6lMwbCy)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment](https://openreview.net/forum?id=v3XXtxWKi6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Safe RLHF: Safe Reinforcement Learning from Human Feedback](https://openreview.net/forum?id=TyFrPOKYXw)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[SALMON: Self-Alignment with Principle-Following Reward Models](https://openreview.net/forum?id=xJbsmB8UMx)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Self-Alignment with Instruction Backtranslation](https://openreview.net/forum?id=1oijHJBRsT)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Oral)](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
- [2023/09] **[Statistical Rejection Sampling Improves Preference Optimization](https://openreview.net/forum?id=xbjSwwrQOe)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning](https://openreview.net/forum?id=hILVmJ4Uvu)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Urial: Aligning Untuned LLMs with Just the 'Write' Amount of In-Context Learning](https://openreview.net/forum?id=wxJ0eXwwda)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[What happens when you fine-tuning your model? Mechanistic analysis of procedurally generated tasks.](https://openreview.net/forum?id=A0HKeKl4Nl)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning](https://openreview.net/forum?id=BTKAeLqLMw)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/08] **[Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment](https://arxiv.org/abs/2308.05374)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/07] **[BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset](https://arxiv.org/abs/2307.04657)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility](https://arxiv.org/abs/2307.09705)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Chinese](https://img.shields.io/badge/Chinese-87b800)
- [2023/05] **[Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/04] **[Fundamental Limitations of Alignment in Large Language Models](https://arxiv.org/abs/2304.11082)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/04] **[RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](https://arxiv.org/abs/2304.06767)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/10] **[Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values](https://arxiv.org/abs/2210.07652)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
