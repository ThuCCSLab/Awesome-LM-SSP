# C7. Unlearning
- [2024/04] **[LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning](https://arxiv.org/abs/2404.05868)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Digital Forgetting in Large Language Models: A Survey of Unlearning Methods](https://arxiv.org/abs/2404.02062)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/03] **[Localizing Paragraph Memorization in Language Models](https://arxiv.org/abs/2403.19851)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Towards Efficient and Effective Unlearning of Large Language Models for Recommendation](https://arxiv.org/abs/2403.03536)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models](https://arxiv.org/abs/2403.10557)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention](https://arxiv.org/abs/2403.11052)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Guardrail Baselines for Unlearning in LLMs](https://arxiv.org/abs/2403.03329)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Dissecting Language Models: Machine Unlearning via Selective Pruning](https://arxiv.org/abs/2403.01267)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination](https://arxiv.org/abs/2402.10052)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Unlearnable Algorithms for In-context Learning](https://arxiv.org/abs/2402.00751)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Towards Safer Large Language Models through Machine Unlearning](https://arxiv.org/abs/2402.10058)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Rethinking Machine Unlearning for Large Language Models ](https://arxiv.org/abs/2402.08787)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models](https://arxiv.org/abs/2402.05813)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[In-Context Learning Can Re-learn Forbidden Tasks](https://arxiv.org/abs/2402.05723)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/jpmorganchase/l2l-generator-unlearning) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/01] **[TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://locuslab.github.io/tofu/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/kevinyaobytedance/llm_unlearn) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238?s=08)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Can Sensitive Information be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Detecting Pretraining Data from Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://swj0419.github.io/detect-pretrain.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/07] **[Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/03] **[Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2303.07345)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
