# C9. Unlearning
- [2025/08] **[Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[VideoEraser: Concept Erasure in Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.15314)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Video](https://img.shields.io/badge/Video-87b800) ![EMNLP'25](https://img.shields.io/badge/EMNLP'25-f1b800)
- [2025/08] **[Refusal Is Not an Option: Unlearning Safety Alignment of Large Language Models](https://www.usenix.org/system/files/usenixsecurity25-song-minkyoo.pdf)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![USENIX_Security'25](https://img.shields.io/badge/USENIX_Security'25-f1b800)
- [2025/07] **[Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning ](https://arxiv.org/abs/2507.16302)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/07] **[Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/06] **[Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models](https://arxiv.org/abs/2506.17279)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[UCD: Unlearning in LLMs via Contrastive Decoding](https://arxiv.org/abs/2506.12097)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy](https://arxiv.org/abs/2506.00359)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'25](https://img.shields.io/badge/CCS'25-f1b800)
- [2025/05] **[Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models ](https://arxiv.org/abs/2505.24379)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs](https://arxiv.org/abs/2504.08192)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Sparse Autoencoder as a Zero-Shot Classifier for Concept Erasing in Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.09446)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/02] **[Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](https://arxiv.org/abs/2502.05374)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/OPTML-Group/Unlearn-Smooth) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICML'25](https://img.shields.io/badge/ICML'25-f1b800)
- [2025/02] **[Mitigating Sensitive Information Leakage in LLMs4Code through Machine Unlearning](https://arxiv.org/abs/2502.05739)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning](https://arxiv.org/abs/2410.07163)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/OPTML-Group/Unlearn-Simple) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[When Machine Unlearning Meets Retrieval-Augmented Generation (RAG): Keep Secret or Forget Knowledge?](https://arxiv.org/abs/2410.15267)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts](https://arxiv.org/abs/2410.12777)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/10] **[Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models](https://arxiv.org/abs/2410.08074)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/09] **[An Adversarial Perspective on Machine Unlearning for AI Safety](https://arxiv.org/abs/2409.18025)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Forget to Flourish: Leveraging Machine-Unlearning on Pretrained Language Models for Privacy Leakage](https://arxiv.org/abs/2408.17354)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models](https://arxiv.org/abs/2408.10682)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Protecting Privacy Through Approximating Optimal Parameters for Sequence Unlearning in Language Models](https://aclanthology.org/2024.findings-acl.936/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24_(Findings)](https://img.shields.io/badge/ACL'24_(Findings)-f1b800)
- [2024/08] **[Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2408.06621)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Practical Unlearning for Large Language Models](https://arxiv.org/abs/2407.10223)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI](https://arxiv.org/abs/2407.00106)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces](https://arxiv.org/abs/2406.11614)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/yihuaihong/ConceptVectors) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2406.10890)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://rwku-bench.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models](https://arxiv.org/abs/2405.15234)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/04] **[SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/04] **[Espresso: Robust Concept Filtering in Text-to-Image Models](https://arxiv.org/abs/2404.19227)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/04] **[Machine Unlearning in Large Language Models](https://arxiv.org/abs/2404.16841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning](https://arxiv.org/abs/2404.05868)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Digital Forgetting in Large Language Models: A Survey of Unlearning Methods](https://arxiv.org/abs/2404.02062)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/03] **[Challenging Forgets: Unveiling the Worst-case Forget Sets in Machine Unlearning](https://arxiv.org/abs/2403.07362)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/OPTML-Group/Unlearn-WorstCase) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ECCV'24](https://img.shields.io/badge/ECCV'24-f1b800)
- [2024/03] **[MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/03] **[Localizing Paragraph Memorization in Language Models](https://arxiv.org/abs/2403.19851)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Towards Efficient and Effective Unlearning of Large Language Models for Recommendation](https://arxiv.org/abs/2403.03536)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models](https://arxiv.org/abs/2403.10557)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Unveiling and Mitigating Memorization in Text-to-image Diffusion Models through Cross Attention](https://arxiv.org/abs/2403.11052)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Guardrail Baselines for Unlearning in LLMs](https://arxiv.org/abs/2403.03329)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Dissecting Language Models: Machine Unlearning via Selective Pruning](https://arxiv.org/abs/2403.01267)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination](https://arxiv.org/abs/2402.10052)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Unlearnable Algorithms for In-context Learning](https://arxiv.org/abs/2402.00751)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Towards Safer Large Language Models through Machine Unlearning](https://arxiv.org/abs/2402.10058)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/02] **[Rethinking Machine Unlearning for Large Language Models ](https://arxiv.org/abs/2402.08787)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models](https://arxiv.org/abs/2402.05813)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[In-Context Learning Can Re-learn Forbidden Tasks](https://arxiv.org/abs/2402.05723)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Machine Unlearning for Image-to-Image Generative Models](https://arxiv.org/abs/2402.00351)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/jpmorganchase/l2l-generator-unlearning) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/01] **[TOFU: A Task of Fictitious Unlearning for LLMs](https://arxiv.org/abs/2401.06121)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://locuslab.github.io/tofu/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[In-Context Unlearning: Language Models as Few Shot Unlearners](https://arxiv.org/abs/2310.07579)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Large Language Model Unlearning](https://arxiv.org/abs/2310.10683)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/kevinyaobytedance/llm_unlearn) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Unlearn What You Want to Forget: Efficient Unlearning for LLMs](https://arxiv.org/abs/2310.20150)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238?s=08)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Can Sensitive Information be Deleted From LLMs? Objectives for Defending Against Extraction Attacks](https://openreview.net/forum?id=7erlRDoaV8)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Detecting Pretraining Data from Large Language Models](https://openreview.net/forum?id=zWqr3MQuNs)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://swj0419.github.io/detect-pretrain.github.io/) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?](https://openreview.net/forum?id=lm7MRcsFiS)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation](https://openreview.net/forum?id=gn0mIhQGNM)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/07] **[Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions](https://arxiv.org/abs/2307.03941)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/03] **[Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2303.07345)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
