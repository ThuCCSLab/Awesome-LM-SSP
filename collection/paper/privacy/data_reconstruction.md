# C3. Data Reconstruction
- [2024/11] **[Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors](https://arxiv.org/abs/2411.01705)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[Stealing User Prompts from Mixture of Experts ](https://arxiv.org/abs/2410.22884)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Remote Timing Attacks on Efficient Language Model Inference](https://arxiv.org/abs/2410.17175)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Extracting Spatiotemporal Data from Gradients with Large Language Models](https://arxiv.org/abs/2410.16121)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Data Defenses Against Large Language Models](https://arxiv.org/abs/2410.13138)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[PII-Scope: A Benchmark for Training Data PII Leakage Assessment in LLMs](https://arxiv.org/abs/2410.06704)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/10] **[Towards a Theoretical Understanding of Memorization in Diffusion Models](https://arxiv.org/abs/2410.02467)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/09] **[Extracting Memorized Training Data via Decomposition](https://arxiv.org/abs/2409.12367)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Prompt Obfuscation for Large Language Models](https://arxiv.org/abs/2409.11026)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/08] **[Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries](https://aclanthology.org/2024.acl-long.230/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/coffree0123/TEIA) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/08] **[Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models](https://arxiv.org/abs/2408.02416)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?](https://arxiv.org/abs/2407.16607)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Was it Slander? Towards Exact Inversion of Generative Language Models](https://arxiv.org/abs/2407.11059)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Towards More Realistic Extraction Attacks: An Adversarial Perspective](https://arxiv.org/abs/2407.02596)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding](https://arxiv.org/abs/2407.02943)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/06] **[Extracting Training Data from Unconditional Diffusion Models](https://arxiv.org/abs/2406.12752)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[Is Diffusion Model Safe? Severe Data Leakage via Gradient-Guided Diffusion Model](https://arxiv.org/abs/2406.09484)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[Raccoon: Prompt Extraction Benchmark of LLM-Integrated Applications](https://arxiv.org/abs/2406.06737)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24_(Findings)](https://img.shields.io/badge/ACL'24_(Findings)-f1b800)
- [2024/05] **[Extracting Prompts by Inverting LLM Outputs](https://arxiv.org/abs/2405.15012)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[PLeak: Prompt Leaking Attacks against Large Language Model Applications](https://arxiv.org/abs/2405.06823)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/05] **[Special Characters Attack: Toward Scalable Training Data Extraction From Large Language Models](https://arxiv.org/abs/2405.05990)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Could It Be Generated? Towards Practical Analysis of Memorization in Text-To-Image Diffusion Models](https://arxiv.org/abs/2405.05846)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/04] **[Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions](https://arxiv.org/abs/2404.16251)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images](https://arxiv.org/abs/2404.13784)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/03] **[Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs](https://arxiv.org/abs/2403.04801)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[PRSA: Prompt Reverse Stealing Attacks against Large Language Models](https://arxiv.org/abs/2402.19200)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2402.17840)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Pandora's White-Box: Increased Training Data Leakage in Open LLMs](https://arxiv.org/abs/2402.17012)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Large Language Models are Advanced Anonymizers](https://arxiv.org/abs/2402.13846)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Prompt Stealing Attacks Against Large Language Models](https://arxiv.org/abs/2402.12959)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Conversation Reconstruction Attack Against GPT Models ](https://arxiv.org/abs/2402.02987)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Text Embedding Inversion Attacks on Multilingual Language Models](https://arxiv.org/abs/2401.12192)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://huggingface.co/yiyic) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Intriguing Properties of Data Attribution on Diffusion Models](https://openreview.net/forum?id=vKViCoKGcB)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Teach LLMs to Phish: Stealing Private Information from Language Models](https://openreview.net/forum?id=qo21ZlfNu6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Language Model Inversion](https://arxiv.org/abs/2311.13647)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/07] **[Effective Prompt Extraction from Language Models](https://arxiv.org/abs/2307.06865v3)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/02] **[Prompt Stealing Attacks Against Text-to-Image Generation Models](https://arxiv.org/abs/2302.09923)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![USENIX_Security'24](https://img.shields.io/badge/USENIX_Security'24-f1b800)
- [2023/01] **[Extracting Training Data from Diffusion Models](https://arxiv.org/abs/2301.13188)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![USENIX_Security'23](https://img.shields.io/badge/USENIX_Security'23-f1b800)
- [2020/12] **[Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![USENIX_Security'21](https://img.shields.io/badge/USENIX_Security'21-f1b800)
