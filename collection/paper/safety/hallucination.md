# A7. Hallucination
- [2024/04] **[Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs](https://arxiv.org/abs/2404.09971)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Reducing hallucination in structured outputs via Retrieval-Augmented Generation](https://arxiv.org/abs/2404.08189)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/04] **[PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics](https://arxiv.org/abs/2404.04722)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding](https://arxiv.org/abs/2403.18715)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/03] **[HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://arxiv.org/abs/2310.14566)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/tianyi-lab/HallusionBench) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![CVPR'24](https://img.shields.io/badge/CVPR'24-f1b800)
- [2024/03] **[The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions](https://arxiv.org/abs/2403.09743)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases](https://arxiv.org/abs/2403.10446)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/03] **[Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics](https://arxiv.org/abs/2403.08904)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts](https://arxiv.org/abs/2403.07556)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/ictnlp/TACS) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models](https://arxiv.org/abs/2403.06448)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Tell me the truth: A system to measure the trustworthiness of Large Language Models](https://arxiv.org/abs/2403.04964)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models](https://arxiv.org/abs/2403.05266)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/03] **[HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild](https://arxiv.org/abs/2403.04307)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification](https://arxiv.org/abs/2403.04696)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[In Search of Truth: An Interrogation Approach to Hallucination Detection](https://arxiv.org/abs/2403.02889)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2403.00896)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/02] **[Reducing Hallucinations in Entity Abstract Summarization with Facts-Template Decomposition](https://arxiv.org/abs/2402.18873)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space](https://arxiv.org/abs/2402.17811)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting](https://arxiv.org/abs/2402.10412)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Comparing Hallucination Detection Metrics for Multilingual Generation](https://arxiv.org/abs/2402.10496)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Strong hallucinations from negation and how to fix them](https://arxiv.org/abs/2402.10543)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2402.10612)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance](https://arxiv.org/abs/2402.08680)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate](https://arxiv.org/abs/2402.07401)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Understanding the Effects of Iterative Prompting on Truthfulness](https://arxiv.org/abs/2402.06625)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Is it Possible to Edit Large Language Models Robustly?](https://arxiv.org/abs/2402.05827)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/xbmxb/edit_analysis) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models](https://arxiv.org/abs/2402.03181)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/01] **[Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment](https://arxiv.org/abs/2401.10768)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/fanqiwan/KCA) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Large Language Models are Null-Shot Learners](https://arxiv.org/abs/2401.08273)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Model Editing Can Hurt General Abilities of Large Language Models](https://arxiv.org/abs/2401.04700)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/JasonForJoy/Model-Editing-Hurt) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty](https://arxiv.org/abs/2401.06730)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Seven Failure Points When Engineering a Retrieval Augmented Generation System](https://arxiv.org/abs/2401.05856)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2023/12] **[DelucionQA: Detecting Hallucinations in Domain-specific Question Answering](https://arxiv.org/abs/2312.05200)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/12] **[Improving Factual Error Correction by Learning to Inject Factual Errors](https://arxiv.org/abs/2312.07049)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback](https://arxiv.org/abs/2312.00849)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation](https://arxiv.org/abs/2312.09085)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models](https://arxiv.org/abs/2311.09210)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination](https://arxiv.org/abs/2311.15548)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus](https://arxiv.org/abs/2311.13230)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23](https://img.shields.io/badge/EMNLP'23-f1b800)
- [2023/11] **[Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges](https://arxiv.org/abs/2311.03287)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/11] **[Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting](https://arxiv.org/abs/2311.13314)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation](https://arxiv.org/abs/2311.15296)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![_Chinese](https://img.shields.io/badge/_Chinese-87b800)
- [2023/11] **[When Large Language Models contradict humans? Large Language Models' Sycophantic Behaviour](https://arxiv.org/abs/2311.09410)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Calibrated Language Models Must Hallucinate](https://arxiv.org/abs/2311.14648)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://arxiv.org/abs/2310.05253)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/10] **[Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity](https://arxiv.org/abs/2310.07521)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/09] **[Analyzing and Mitigating Object Hallucination in Large Vision-Language Models](https://openreview.net/forum?id=oZDJKTlOUe)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning](https://openreview.net/forum?id=mMaQvkMzDi)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models](https://openreview.net/forum?id=3TO3TtnOFl)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](https://openreview.net/forum?id=cPgh4gWZlz)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding](https://openreview.net/forum?id=4L0xnS4GQM)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Compressing LLMs: The Truth is Rarely Pure and Never Simple](https://openreview.net/forum?id=B9klVS7Ddk)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Conformal Language Modeling](https://openreview.net/forum?id=pzUhfQ74c5)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing](https://openreview.net/forum?id=Sx038qxjek)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation](https://openreview.net/forum?id=ITq4ZRUT4a)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Do Large Language Models Know about Facts?](https://openreview.net/forum?id=9OevMUdods)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://openreview.net/forum?id=Th6NyL07na)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Ferret: Refer and Ground Anything Anywhere at Any Granularity](https://openreview.net/forum?id=2msbbX3ydD)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Fine-Tuning Language Models for Factuality](https://openreview.net/forum?id=WPZ2yPag4K)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection](https://openreview.net/forum?id=Zj12nzlQbz)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Lightweight Language Model Calibration for Open-ended Question Answering with Varied Answer Lengths](https://openreview.net/forum?id=jH67LHVOIO)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[MetaGPT: Meta Programming for Multi-Agent Collaborative Framework](https://openreview.net/forum?id=VtmBAGCN7o)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800) ![ICLR'24_(Oral)](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
- [2023/09] **[Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://openreview.net/forum?id=J44HfH4JCg)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[RAPPER: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering](https://openreview.net/forum?id=bshfchPM9H)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning](https://openreview.net/forum?id=ZGNWW7xZ6Q)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Self-Contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation](https://openreview.net/forum?id=EmQSOi1X2f)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Supervised Knowledge Makes Large Language Models Better In-context Learners](https://openreview.net/forum?id=bAMPOUF227)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Teaching Language Models to Hallucinate Less with Synthetic Tasks](https://openreview.net/forum?id=xpw7V0P136)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Teaching Large Language Models to Self-Debug](https://openreview.net/forum?id=KuPixIqPiq)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[The Reasonableness Behind Unreasonable Translation Capability of Large Language Model](https://openreview.net/forum?id=3KDbIWT26J)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://openreview.net/forum?id=nnVO1PvbTv)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Unveiling and Manipulating Prompt Influence in Large Language Models](https://openreview.net/forum?id=ap1ByuwQrX)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/08] **[Simple synthetic data reduces sycophancy in large language models](https://arxiv.org/abs/2308.03958)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation](https://arxiv.org/abs/2307.03987)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models](https://arxiv.org/abs/2307.01379)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/jinhaoduan/shifting-attention-to-relevance) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[Explore, Establish, Exploit: Red Teaming Language Models from Scratch](https://arxiv.org/abs/2306.09442)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[Inference-Time Intervention: Eliciting Truthful Answers from a Language Model](https://arxiv.org/abs/2306.03341)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Fact-Checking Complex Claims with Program-Guided Reasoning](https://arxiv.org/abs/2305.12744)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'23](https://img.shields.io/badge/ACL'23-f1b800)
- [2023/05] **[HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![EMNLP'23](https://img.shields.io/badge/EMNLP'23-f1b800)
- [2023/05] **[Improving Factuality and Reasoning in Language Models through Multiagent Debate](https://arxiv.org/abs/2305.14325)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models](https://arxiv.org/abs/2305.13712)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Mitigating Language Model Hallucination with Interactive Question-Knowledge Alignment](https://arxiv.org/abs/2305.13669)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Sources of Hallucination by Large Language Models on Inference Tasks](https://arxiv.org/abs/2305.14552)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/05] **[Trusting Your Evidence: Hallucinate Less with Context-aware Decoding](https://arxiv.org/abs/2305.14739)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/04] **[In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT](https://arxiv.org/abs/2304.08979)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/03] **[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23](https://img.shields.io/badge/EMNLP'23-f1b800)
- [2023/02] **[A Categorical Archive of ChatGPT Failures](https://arxiv.org/abs/2302.03494)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/02] **[Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/pengbaolin/LLM-Augmenter) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/02] **[Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'22](https://img.shields.io/badge/NeurIPS'22-f1b800)
- [2022/02] **[Survey of Hallucination in Natural Language Generation](https://arxiv.org/abs/2202.03629)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
