# A8. Toxicity
- [2024/05] **[Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/abs/2405.09948)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models](https://arxiv.org/abs/2405.09373)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images](https://arxiv.org/abs/2405.03486)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/04] **[SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models](https://arxiv.org/abs/2403.18957)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![USENIX_Security'24](https://img.shields.io/badge/USENIX_Security'24-f1b800)
- [2024/03] **[MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://arxiv.org/abs/2403.14652)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![MM'24](https://img.shields.io/badge/MM'24-f1b800)
- [2024/03] **[Risk and Response in Large Language Models: Evaluating Key Threat Categories](https://arxiv.org/abs/2403.14988)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards](https://arxiv.org/abs/2403.13213)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation](https://arxiv.org/abs/2403.12075)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention](https://arxiv.org/abs/2403.09795)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection](https://arxiv.org/abs/2403.08035)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models](https://arxiv.org/abs/2403.03893)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[DPP-Based Adversarial Prompt Searching for Lanugage Model](https://arxiv.org/abs/2403.00292)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[LLMGuard: Guarding Against Unsafe LLM Behavior](https://arxiv.org/abs/2403.00826)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?](https://arxiv.org/abs/2402.15238)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language](https://arxiv.org/abs/2402.13818)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content](https://arxiv.org/abs/2402.13926)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Zero shot VLMs for hate meme detection: Are we there yet?](https://arxiv.org/abs/2402.12198)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Universal Prompt Optimizer for Safe Text-to-Image Generation](https://arxiv.org/abs/2402.10882)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/02] **[Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA](https://arxiv.org/abs/2402.06549)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/01] **[Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media](https://arxiv.org/abs/2401.10841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/palomapiot/metahate/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2312.15099)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![S&P'24](https://img.shields.io/badge/S&P'24-f1b800)
- [2023/12] **[Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](https://arxiv.org/abs/2312.08303)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/12] **[Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Unveiling the Implicit Toxicity in Large Language Models](https://arxiv.org/abs/2311.17391)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23](https://img.shields.io/badge/EMNLP'23-f1b800)
- [2023/10] **[All Languages Matter: On the Multilingual Safety of Large Language Models](https://arxiv.org/abs/2310.00905)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts](https://arxiv.org/abs/2310.16613)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/09] **[(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild](https://openreview.net/forum?id=Bl8u7ZRlbM)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![New_dataset](https://img.shields.io/badge/New_dataset-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Controlled Text Generation via Language Model Arithmetic](https://openreview.net/forum?id=SLw9fp4yI6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Curiosity-driven Red-teaming for Large Language Models](https://openreview.net/forum?id=4KqkizXgXU)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset](https://openreview.net/forum?id=BOfDKxfwt0)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![New_dataset](https://img.shields.io/badge/New_dataset-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Understanding Catastrophic Forgetting in Language Models via Implicit Inference](https://openreview.net/forum?id=VrHiF2hsrm)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://openreview.net/forum?id=6bcAD6g688)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[What's In My Big Data?](https://openreview.net/forum?id=RvfPnOkPV4)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/08] **[Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/08] **[You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content](https://arxiv.org/abs/2308.05596)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![S&P'24](https://img.shields.io/badge/S&P'24-f1b800)
- [2023/05] **[Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection](https://arxiv.org/abs/2305.13276)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models](https://arxiv.org/abs/2305.13873)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'23](https://img.shields.io/badge/CCS'23-f1b800)
- [2023/04] **[Toxicity in ChatGPT: Analyzing Persona-assigned Language Models](https://arxiv.org/abs/2304.05335)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/02] **[Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models](https://arxiv.org/abs/2302.07388)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EACL'23](https://img.shields.io/badge/EACL'23-f1b800)
- [2023/02] **[Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://arxiv.org/abs/2302.07736)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![WWW'23_(Companion_Volume)](https://img.shields.io/badge/WWW'23_(Companion_Volume)-f1b800)
- [2022/12] **[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/12] **[On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://arxiv.org/abs/2212.08061)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'23](https://img.shields.io/badge/ACL'23-f1b800)
- [2022/10] **[Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization](https://arxiv.org/abs/2210.04492)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'23](https://img.shields.io/badge/ICLR'23-f1b800)
- [2022/05] **[Toxicity Detection with Generative Prompt-based Inference](https://arxiv.org/abs/2205.12390)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/04] **[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/03] **[ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'22](https://img.shields.io/badge/ACL'22-f1b800)
- [2020/09] **[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/allenai/real-toxicity-prompts?tab=readme-ov-file) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'20_(Findings)](https://img.shields.io/badge/EMNLP'20_(Findings)-f1b800)
