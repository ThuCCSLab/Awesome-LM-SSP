# B1. Adversarial Examples
- [2024/05] **[SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models](https://arxiv.org/abs/2405.08317)** ![SLM](https://img.shields.io/badge/SLM-)
- [2024/04] **[Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models](https://arxiv.org/abs/2404.15081)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets](https://arxiv.org/abs/2403.20056)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Improving the Robustness of Large Language Models via Consistency Alignment](https://arxiv.org/abs/2403.14221)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![LREC-COLING‘24](https://img.shields.io/badge/LREC-COLING‘24-f1b800)
- [2024/03] **[Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions](https://arxiv.org/abs/2403.12077)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator](https://arxiv.org/abs/2403.11833)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Transferable Multimodal Attack on Vision-Language Pre-training Models ](https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![S&P'24](https://img.shields.io/badge/S&P'24-f1b800)
- [2024/03] **[AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions ](https://arxiv.org/abs/2403.09346)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2024/03] **[The Impact of Quantization on the Robustness of Transformer-based Text Classifiers](https://arxiv.org/abs/2403.05365)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Fast Adversarial Attacks on Language Models In One GPU Minute ](https://arxiv.org/abs/2402.15570)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/vinusankars/BEAST) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Stealthy Attack on Large Language Model based Recommendation](https://arxiv.org/abs/2402.14836)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Recommendation](https://img.shields.io/badge/Recommendation-87b800)
- [2024/02] **[BSPA: Exploring Black-box Stealthy Prompt Attacks against Image Generators](https://arxiv.org/abs/2402.15218)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/02] **[Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images](https://arxiv.org/abs/2402.14899)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative](https://arxiv.org/abs/2402.14859)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/ChengshuaiZhao0/The-Wolf-Within) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](https://arxiv.org/abs/2402.14016)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation](https://arxiv.org/abs/2402.12100)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/02] **[Exploring the Adversarial Capabilities of Large Language Models ](https://arxiv.org/abs/2402.09132)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models](https://arxiv.org/abs/2402.07179)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Adversarial Text Purification: A Large Language Model Approach for Defense](https://arxiv.org/abs/2402.06655)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![PAKDD_2024](https://img.shields.io/badge/PAKDD_2024-f1b800)
- [2024/02] **[Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors](https://arxiv.org/abs/2402.01369)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/ydc123/MMP-Attack) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/01] **[Revealing Vulnerabilities in Stable Diffusion via Targeted Attacks](https://arxiv.org/abs/2401.08725)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/datar001/Revealing-Vulnerabilities-in-Stable-Diffusion-via-Targeted-Attacks) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/01] **[Exploring Adversarial Attacks against Latent Diffusion Model from the Perspective of Adversarial Transferability](https://arxiv.org/abs/2401.07087)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/01] **[Adversarial Examples are Misaligned in Diffusion Model Manifolds](https://arxiv.org/abs/2401.06637)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/01] **[INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models](https://arxiv.org/abs/2312.01886)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/12] **[On the Robustness of Large Multimodal Models Against Image Adversarial Attacks](https://arxiv.org/abs/2312.03777)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/12] **[Causality Analysis for Evaluating the Security of Large Language Models](https://arxiv.org/abs/2312.07876)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Hijacking Context in Large Multi-modal Models](https://arxiv.org/abs/2312.07553)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/11] **[Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention](https://arxiv.org/abs/2311.17400)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![NDSS'24](https://img.shields.io/badge/NDSS'24-f1b800)
- [2023/11] **[Unveiling Safety Vulnerabilities of Large Language Models](https://arxiv.org/abs/2311.04124)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?](https://arxiv.org/abs/2312.00084)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/11] **[DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial Purification](https://arxiv.org/abs/2311.16124)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/kangmintong/DiffAttack) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/11] **[How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs](https://arxiv.org/abs/2311.16101)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/UCSC-VLAA/vllm-safety-benchmark) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2023/10] **[Misusing Tools in Large Language Models With Visual Adversarial Examples](https://arxiv.org/abs/2310.03185)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/09] **[Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images](https://openreview.net/forum?id=BteuUysuXX)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/KuofengGao/Verbose_Images) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models](https://openreview.net/forum?id=nc5GgFAvtk)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[An LLM can Fool Itself: A Prompt-Based Adversarial Attack](https://openreview.net/forum?id=VVgGbB9TNV)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Language Model Detectors Are Easily Optimized Against](https://openreview.net/forum?id=4eJDMjYZZG)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Leveraging Optimization for Adaptive Attacks on Image Watermarks](https://openreview.net/forum?id=O9PArxKLe1)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Training Socially Aligned Language Models on Simulated Social Interactions](https://openreview.net/forum?id=NddKiWtdUm)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[How Robust is Google's Bard to Adversarial Image Attacks?](https://arxiv.org/abs/2309.11751)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/thu-ml/Attack-Bard) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/09] **[Image Hijacks: Adversarial Images Can Control Generative Models at Runtime](https://arxiv.org/abs/2309.00236)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/euanong/image-hijacks) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/08] **[Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2308.11804)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/ebagdasa/adversarial_illusions) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/08] **[On the Adversarial Robustness of Multi-Modal Foundation Models](https://arxiv.org/abs/2308.10741)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICCV'23_(AROW_Workshop)](https://img.shields.io/badge/ICCV'23_(AROW_Workshop)-f1b800)
- [2023/08] **[Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models](https://arxiv.org/abs/2308.07847)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[Certified Robustness for Large Language Models with Self-Denoising](https://arxiv.org/abs/2307.07171)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/06] **[Adversarial Examples in the Age of ChatGPT](http://spylab.ai/blog/chatbot-adversarial-examples/)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Blog](https://img.shields.io/badge/Blog-f1b800)
- [2023/06] **[Are Aligned Neural Networks Adversarially Aligned?](https://arxiv.org/abs/2306.15447)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/06] **[PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](https://arxiv.org/abs/2306.04528)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2023/06] **[Stable Diffusion is Unstable](https://arxiv.org/abs/2306.02583)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/06] **[Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation](https://arxiv.org/abs/2306.01902)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/ZhengyueZhao/EUDP) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2023/06] **[Visual Adversarial Examples Jailbreak Large Language Models](https://arxiv.org/abs/2306.13213)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/05] **[Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability](https://arxiv.org/abs/2305.16494)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/xavihart/Diff-PGD) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/05] **[Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility](https://arxiv.org/abs/2305.10235)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[On evaluating adversarial robustness of large vision-language models](https://arxiv.org/abs/2305.16934)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/yunqing-me/AttackVLM) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/03] **[Anti-DreamBooth: Protecting Users from Personalized Text-to-Image Synthesis](https://arxiv.org/abs/2303.15433)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/VinAIResearch/Anti-DreamBooth) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICCV'23](https://img.shields.io/badge/ICCV'23-f1b800)
- [2023/02] **[Large Language Models Can Be Easily Distracted by Irrelevant Context](https://arxiv.org/abs/2302.00093)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'23](https://img.shields.io/badge/ICML'23-f1b800)
- [2023/02] **[On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](https://arxiv.org/abs/2302.12095)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/02] **[Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples](https://arxiv.org/abs/2302.04578)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/mist-project/mist) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICML'23](https://img.shields.io/badge/ICML'23-f1b800)
- [2023/02] **[Raising the Cost of Malicious AI-Powered Image Editing](https://arxiv.org/abs/2302.06588)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/MadryLab/photoguard) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICML'23](https://img.shields.io/badge/ICML'23-f1b800)
- [2023/01] **[On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex](https://arxiv.org/abs/2301.12868)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![CodeGen](https://img.shields.io/badge/CodeGen-87b800) ![EACL'23](https://img.shields.io/badge/EACL'23-f1b800)
- [2022/12] **[Understanding Zero-shot Adversarial Robustness for Large-Scale Model](https://arxiv.org/abs/2212.07016)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/cvlab-columbia/ZSRobust4FoundationModel) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'23](https://img.shields.io/badge/ICLR'23-f1b800)
- [2021/11] **[Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models](https://arxiv.org/abs/2111.02840)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![NeurIPS'21](https://img.shields.io/badge/NeurIPS'21-f1b800)
