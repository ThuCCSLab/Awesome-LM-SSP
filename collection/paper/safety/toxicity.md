# A8. Toxicity
- [2025/08] **[Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/07] **[Hate in Plain Sight: On the Risks of Moderating AI-Generated Hateful Illusions](https://arxiv.org/abs/2507.22617)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICCV'25](https://img.shields.io/badge/ICCV'25-f1b800)
- [2025/07] **[Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities](https://arxiv.org/abs/2507.11155)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![USENIX_Security'25](https://img.shields.io/badge/USENIX_Security'25-f1b800)
- [2025/06] **[PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/05] **[Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/02] **[I know what you MEME! Understanding and Detecting Harmful Memes with Multimodal Large Language Models](https://www.ndss-symposium.org/ndss-paper/i-know-what-you-meme-understanding-and-detecting-harmful-memes-with-multimodal-large-language-models/)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2025/01] **[Distorting Embedding Space for Safety: A Defense Mechanism for Adversarially Robust Diffusion Models](https://arxiv.org/abs/2501.18877)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/01] **[HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns](https://arxiv.org/abs/2501.16750)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![USENIX_Secuirty'25](https://img.shields.io/badge/USENIX_Secuirty'25-f1b800)
- [2025/01] **[T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation](https://arxiv.org/abs/2501.12612)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2025/01] **[PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models](https://arxiv.org/abs/2501.03544)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/01] **[How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models](https://arxiv.org/abs/2501.01741)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[Toxicity Detection towards Adaptability to Changing Perturbations](https://arxiv.org/abs/2412.15267)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding](https://arxiv.org/abs/2411.10329)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/10] **[Soft-Label Integration for Robust Toxicity Classification](https://arxiv.org/abs/2410.14894)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Can a large language model be a gaslighter?](https://arxiv.org/abs/2410.09181)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[On Calibration of LLM-based Guard Models for Reliable Content Moderation](https://arxiv.org/abs/2410.10414)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[SteerDiff: Steering towards Safe Text-to-Image Diffusion Models](https://arxiv.org/abs/2410.02710)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/08] **[DiffZOO: A Purely Query-Based Black-Box Attack for Red-teaming Text-to-Image Generative Model via Zeroth Order Optimization](https://arxiv.org/abs/2408.11071)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/08] **[Efficient Detection of Toxic Prompts in Large Language Models](https://arxiv.org/abs/2408.11727)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[LeCov: Multi-level Testing Criteria for Large Language Models](https://arxiv.org/abs/2408.10474)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Uncertainty-Guided Modal Rebalance for Hateful Memes Detection](https://aclanthology.org/2024.acl-long.239/)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/08] **[Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies ](https://arxiv.org/abs/2408.07728)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/07] **[Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection](https://arxiv.org/abs/2407.21004)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/07] **[Towards Understanding Unsafe Video Generation](https://arxiv.org/abs/2407.12581)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Video](https://img.shields.io/badge/Video-87b800) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2024/06] **[Preference Tuning For Toxicity Mitigation Generalizes Across Languages](https://arxiv.org/abs/2406.16235)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Supporting Human Raters with the Detection of Harmful Content using Large Language Models](https://arxiv.org/abs/2406.12800)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users](https://arxiv.org/abs/2405.19360)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/05] **[Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/abs/2405.09948)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models](https://arxiv.org/abs/2405.09373)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[UnsafeBench: Benchmarking Image Safety Classifiers on Real-World and AI-Generated Images](https://arxiv.org/abs/2405.03486)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/04] **[SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/03] **[Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models](https://arxiv.org/abs/2403.18957)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![USENIX_Security'24](https://img.shields.io/badge/USENIX_Security'24-f1b800)
- [2024/03] **[MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation](https://arxiv.org/abs/2403.14652)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![MM'24](https://img.shields.io/badge/MM'24-f1b800)
- [2024/03] **[Risk and Response in Large Language Models: Evaluating Key Threat Categories](https://arxiv.org/abs/2403.14988)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards](https://arxiv.org/abs/2403.13213)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation](https://arxiv.org/abs/2403.12075)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention](https://arxiv.org/abs/2403.09795)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection](https://arxiv.org/abs/2403.08035)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models](https://arxiv.org/abs/2403.03893)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[DPP-Based Adversarial Prompt Searching for Lanugage Model](https://arxiv.org/abs/2403.00292)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[LLMGuard: Guarding Against Unsafe LLM Behavior](https://arxiv.org/abs/2403.00826)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?](https://arxiv.org/abs/2402.15238)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language](https://arxiv.org/abs/2402.13818)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content](https://arxiv.org/abs/2402.13926)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Zero shot VLMs for hate meme detection: Are we there yet?](https://arxiv.org/abs/2402.12198)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Universal Prompt Optimizer for Safe Text-to-Image Generation](https://arxiv.org/abs/2402.10882)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/02] **[Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework and Semantic-Based Metric](https://arxiv.org/abs/2402.06900)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA](https://arxiv.org/abs/2402.06549)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/01] **[Using LLMs to discover emerging coded antisemitic hate-speech emergence in extremist social media](https://arxiv.org/abs/2401.10841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection](https://arxiv.org/abs/2401.06526)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/palomapiot/metahate/) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity](https://arxiv.org/abs/2401.01967)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2312.15099)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![S&P'24](https://img.shields.io/badge/S&P'24-f1b800)
- [2023/12] **[Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models](https://arxiv.org/abs/2312.08303)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[GTA: Gated Toxicity Avoidance for LM Performance Preservation](https://arxiv.org/abs/2312.06122)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/12] **[Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/facebookresearch/PurpleLlama/tree/main/Llama-Guard) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Unveiling the Implicit Toxicity in Large Language Models](https://arxiv.org/abs/2311.17391)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23](https://img.shields.io/badge/EMNLP'23-f1b800)
- [2023/10] **[All Languages Matter: On the Multilingual Safety of Large Language Models](https://arxiv.org/abs/2310.00905)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts](https://arxiv.org/abs/2310.16613)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2023/09] **[(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild](https://openreview.net/forum?id=Bl8u7ZRlbM)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![New_dataset](https://img.shields.io/badge/New_dataset-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Controlled Text Generation via Language Model Arithmetic](https://openreview.net/forum?id=SLw9fp4yI6)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Curiosity-driven Red-teaming for Large Language Models](https://openreview.net/forum?id=4KqkizXgXU)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[RealChat-1M: A Large-Scale Real-World LLM Conversation Dataset](https://openreview.net/forum?id=BOfDKxfwt0)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![New_dataset](https://img.shields.io/badge/New_dataset-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Understanding Catastrophic Forgetting in Language Models via Implicit Inference](https://openreview.net/forum?id=VrHiF2hsrm)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models](https://openreview.net/forum?id=6bcAD6g688)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[What's In My Big Data?](https://openreview.net/forum?id=RvfPnOkPV4)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/08] **[Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](https://arxiv.org/abs/2308.13387)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/08] **[You Only Prompt Once: On the Capabilities of Prompt Learning on Large Language Models to Tackle Toxic Content](https://arxiv.org/abs/2308.05596)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![S&P'24](https://img.shields.io/badge/S&P'24-f1b800)
- [2023/05] **[Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection](https://arxiv.org/abs/2305.13276)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models](https://arxiv.org/abs/2305.13873)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'23](https://img.shields.io/badge/CCS'23-f1b800)
- [2023/04] **[Toxicity in ChatGPT: Analyzing Persona-assigned Language Models](https://arxiv.org/abs/2304.05335)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/02] **[Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models](https://arxiv.org/abs/2302.07388)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EACL'23](https://img.shields.io/badge/EACL'23-f1b800)
- [2023/02] **[Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech](https://arxiv.org/abs/2302.07736)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![WWW'23_(Companion_Volume)](https://img.shields.io/badge/WWW'23_(Companion_Volume)-f1b800)
- [2022/12] **[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/12] **[On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning](https://arxiv.org/abs/2212.08061)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'23](https://img.shields.io/badge/ACL'23-f1b800)
- [2022/10] **[Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization](https://arxiv.org/abs/2210.04492)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'23](https://img.shields.io/badge/ICLR'23-f1b800)
- [2022/05] **[Toxicity Detection with Generative Prompt-based Inference](https://arxiv.org/abs/2205.12390)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/04] **[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2204.05862)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2022/03] **[ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection](https://arxiv.org/abs/2203.09509)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'22](https://img.shields.io/badge/ACL'22-f1b800)
- [2020/09] **[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/allenai/real-toxicity-prompts?tab=readme-ov-file) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'20_(Findings)](https://img.shields.io/badge/EMNLP'20_(Findings)-f1b800)
