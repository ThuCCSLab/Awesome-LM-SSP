# A5. Fairness
- [2024/06] **[GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models](https://arxiv.org/abs/2405.03098)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models](https://arxiv.org/abs/2403.14633)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Protected group bias and stereotypes in Large Language Models](https://arxiv.org/abs/2403.14727)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Locating and Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2403.14409)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework](https://arxiv.org/abs/2403.08743)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution](https://arxiv.org/abs/2403.03121)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **["Flex Tape Can't Fix That": Bias and Misinformation in Edited Language Models](https://arxiv.org/abs/2403.00180)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Gender Bias in Large Language Models across Multiple Languages](https://arxiv.org/abs/2403.00277)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[FairBelief - Assessing Harmful Beliefs in Language Models](https://arxiv.org/abs/2402.17389)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[What's in a Name? Auditing Large Language Models for Race and Gender Bias](https://arxiv.org/abs/2402.14875)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Measuring Social Biases in Masked Language Models by Proxy of Prediction Quality](https://arxiv.org/abs/2402.13954)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One](https://arxiv.org/abs/2402.12150)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Disclosure and Mitigation of Gender Bias in LLMs](https://arxiv.org/abs/2402.11190)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models](https://arxiv.org/abs/2402.10436)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting](https://arxiv.org/abs/2401.15585)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Gender Bias in Machine Translation and The Era of Large Language Models](https://arxiv.org/abs/2401.10016)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Leveraging Biases in Large Language Models: "bias-kNN'' for Effective Few-Shot Learning](https://arxiv.org/abs/2401.09783)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICASSP'24](https://img.shields.io/badge/ICASSP'24-f1b800)
- [2024/01] **[Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation](https://arxiv.org/abs/2401.06310)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2023/12] **[GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models](https://arxiv.org/abs/2311.09428)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity](https://arxiv.org/abs/2311.18580)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[ROBBIE: Robust Bias Evaluation of Large Generative Language Models](https://arxiv.org/abs/2311.18140)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23](https://img.shields.io/badge/EMNLP'23-f1b800)
- [2023/10] **[Im not Racist but...: Discovering Bias in the Internal Knowledge of Large Language Models](https://arxiv.org/abs/2310.08780)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Investigating the Fairness of Large Language Models for Predictions on Tabular Data](https://arxiv.org/abs/2310.14607)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Kelly is a Warm Person, Joseph is a Role Model: Gender Biases in LLM-Generated Reference Letters](https://arxiv.org/abs/2310.09219)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/09] **[Achieving Fairness in Multi-Agent MDP Using Reinforcement Learning](https://openreview.net/forum?id=yoVq2BGQdP)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs](https://openreview.net/forum?id=kGteeZ18Ir)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[FairVLM: Mitigating Bias In Pre-Trained Vision-Language Models](https://openreview.net/forum?id=HXoq9EqR9e)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Finetuning Text-to-Image Diffusion Models for Fairness](https://openreview.net/forum?id=hnrB5YHoYu)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICLR'24_(Oral)](https://img.shields.io/badge/ICLR'24_(Oral)-f1b800)
- [2023/09] **[The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models](https://openreview.net/forum?id=SQGUDc9tC8)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Bias and Fairness in Chatbots: An Overview](https://arxiv.org/abs/2309.08836)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Bias and Fairness in Large Language Models: A Survey](https://arxiv.org/abs/2309.00770)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2023/09] **[People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review](https://arxiv.org/abs/2309.14504)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/08] **[FairBench: A Four-Stage Automatic Framework for Detecting Stereotypes and Biases in Large Language Models](https://arxiv.org/abs/2308.10397)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2023/08] **[Gender bias and stereotypes in Large Language Models](https://arxiv.org/abs/2308.14921)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![CI'23](https://img.shields.io/badge/CI'23-f1b800)
- [2023/07] **[Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models](https://arxiv.org/abs/2307.00101)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[Knowledge of cultural moral norms in large language models](https://arxiv.org/abs/2306.01857)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'23](https://img.shields.io/badge/ACL'23-f1b800)
- [2023/06] **[WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models](https://arxiv.org/abs/2306.15087)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800) ![ACL'23](https://img.shields.io/badge/ACL'23-f1b800)
- [2023/05] **[BiasAsker: Measuring the Bias in Conversational AI System](https://arxiv.org/abs/2305.12434)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![FSE'23](https://img.shields.io/badge/FSE'23-f1b800)
- [2023/05] **[Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation](https://arxiv.org/abs/2305.07609)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/jizhi-zhang/FaiRLLM) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Recsys'23](https://img.shields.io/badge/Recsys'23-f1b800)
- [2023/05] **[Large Language Models are not Fair Evaluators](https://arxiv.org/abs/2305.17926)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/i-Eval/FairEval) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Uncovering and Quantifying Social Biases in Code Generation](https://arxiv.org/abs/2305.15377)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![codeGen](https://img.shields.io/badge/codeGen-87b800)
- [2022/09] **[Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis](https://arxiv.org/abs/2209.08891)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![JAIR'23](https://img.shields.io/badge/JAIR'23-f1b800)
- [2022/09] **[Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://arxiv.org/abs/2209.12106)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'23_(student)](https://img.shields.io/badge/ACL'23_(student)-f1b800)
- [2022/05] **[Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts](https://aclanthology.org/2022.acl-long.72/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/Irenehere/Auto-Debias) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ACL'22](https://img.shields.io/badge/ACL'22-f1b800)
- [2022/03] **[Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://arxiv.org/abs/2203.12574)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'22_(Findings)](https://img.shields.io/badge/ACL'22_(Findings)-f1b800)
- [2021/04] **[Mitigating Political Bias in Language Models Through Reinforced Calibration](https://arxiv.org/abs/2104.14795)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AAAI'21](https://img.shields.io/badge/AAAI'21-f1b800)
- [2021/02] **[Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models](https://arxiv.org/abs/2102.04130)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'21](https://img.shields.io/badge/NeurIPS'21-f1b800)
- [2021/01] **[Persistent Anti-Muslim Bias in Large Language Models](https://arxiv.org/abs/2101.05783)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AIES'21](https://img.shields.io/badge/AIES'21-f1b800)
