# C0. General
- [2025/05] **[Automated Profile Inference with Language Model Agents](https://arxiv.org/abs/2505.12402)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[A Survey on Privacy Risks and Protection in Large Language Models](https://arxiv.org/abs/2505.01976)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model](https://arxiv.org/abs/2504.19373)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/02] **[The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text](https://arxiv.org/abs/2502.14921)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation](https://arxiv.org/abs/2502.15233)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System](https://arxiv.org/abs/2502.11358)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage](https://arxiv.org/abs/2412.05734)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/11] **[Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents](https://arxiv.org/abs/2411.01344)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/10] **[Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents](https://arxiv.org/abs/2410.11906)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/08] **[LLM-PBE: Assessing Data Privacy in Large Language Models](https://arxiv.org/abs/2408.12787)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Data Exposure from LLM Apps: An In-depth Investigation of OpenAI's GPTs](https://arxiv.org/abs/2408.13247)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Mitigating Privacy Seesaw in Large Language Models: Augmented Privacy Neuron Editing via Activation Patching](https://aclanthology.org/2024.findings-acl.315/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/flamewei123/APNEAP-) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![ACL'24_(Findings)](https://img.shields.io/badge/ACL'24_(Findings)-f1b800)
- [2024/08] **[Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://aclanthology.org/2024.acl-long.741/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/08] **[Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions](https://arxiv.org/abs/2408.05212)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/abs/2406.14773)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/05] **[Learnable Privacy Neurons Localization in Language Models](https://aclanthology.org/2024.acl-short.25/)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'24](https://img.shields.io/badge/ACL'24-f1b800)
- [2024/05] **[Information Leakage from Embedding in Large Language Models](https://arxiv.org/abs/2405.11916)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Air Gap: Protecting Privacy-Conscious Conversational Agents](https://arxiv.org/abs/2405.05175)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/04] **[Can LLMs get help from other LLMs without revealing private information?](https://arxiv.org/abs/2404.01041)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk](https://arxiv.org/html/2403.09450v1)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps](https://arxiv.org/abs/2403.09562)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Visual Privacy Auditing with Diffusion Models](https://arxiv.org/abs/2403.07588)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Analysis of Privacy Leakage in Federated Large Language Models](https://arxiv.org/abs/2403.04784)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![FL](https://img.shields.io/badge/FL-87b800)
- [2024/03] **[CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following](https://arxiv.org/abs/2403.03129)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)](https://aclanthology.org/2024.findings-acl.267)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800) ![ACL'24_(Findings)](https://img.shields.io/badge/ACL'24_(Findings)-f1b800)
- [2024/01] **[Excuse me, sir? Your language model is leaking (information)](https://arxiv.org/abs/2401.10360)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning](https://arxiv.org/abs/2310.11397)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[Beyond Memorization: Violating Privacy via Inference with Large Language Models](https://openreview.net/forum?id=kmn0BhQk7p)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory](https://openreview.net/forum?id=c93SBwz1Ma)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/09] **[Privacy Side Channels in Machine Learning Systems](https://arxiv.org/abs/2309.05610)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Side_channel](https://img.shields.io/badge/Side_channel-87b800)
- [2023/07] **[ProPILE: Probing Privacy Leakage in Large Language Models](https://arxiv.org/abs/2307.01881)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/05] **[ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review](https://arxiv.org/abs/2305.03123)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
