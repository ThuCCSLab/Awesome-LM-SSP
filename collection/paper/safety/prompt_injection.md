# A7. Prompt Injection
- [2025/05] **[Defending against Indirect Prompt Injection by Instruction Detection](https://arxiv.org/abs/2505.06311)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/05] **[OET: Optimization-based prompt injection Evaluation Toolkit](https://arxiv.org/abs/2505.00843)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2504.21228)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/04] **[Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction](https://arxiv.org/abs/2504.20472)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression](https://arxiv.org/abs/2504.20493)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection](https://arxiv.org/abs/2504.16125)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks](https://arxiv.org/abs/2504.11358)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[StruPhantom: Evolutionary Injection Attacks on Black-Box Tabular Agents Powered by Large Language Models](https://arxiv.org/abs/2504.09841)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Separator Injection Attack: Uncovering Dialogue Biases in Large Language Models Caused by Role Separators](https://arxiv.org/abs/2504.05689)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Encrypted Prompt: Securing LLM Applications Against Unauthorized Actions](https://arxiv.org/abs/2503.23250)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Defeating Prompt Injections by Design](https://arxiv.org/abs/2503.18813)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents](https://arxiv.org/abs/2503.00061)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[Prompt Inject Detection with Generative Explanation as an Investigative Tool](https://arxiv.org/abs/2502.11006)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/02] **[RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage](https://arxiv.org/abs/2502.08966)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/02] **[MELON: Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison](https://arxiv.org/abs/2502.05174)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/01] **[PromptShield: Deployable Detection for Prompt Injection Attacks](https://arxiv.org/abs/2501.15145)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2025/01] **[Computing Optimization-Based Prompt Injections Against Closed-Weights Models By Misusing a Fine-Tuning API](https://arxiv.org/abs/2501.09798)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2412.16682)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/12] **[Towards Action Hijacking of Large Language Model-based Agent](https://arxiv.org/abs/2412.10807)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/12] **[Trust No AI: Prompt Injection Along The CIA Security Triad](https://arxiv.org/abs/2412.06090)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[Universal and Context-Independent Triggers for Precise Control of LLM Outputs](https://arxiv.org/abs/2411.14738)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/11] **[Attention Tracker: Detecting Prompt Injection Attacks in LLMs](https://arxiv.org/abs/2411.00348)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/11] **[Defense Against Prompt Injection Attack by Leveraging Attack Techniques](https://arxiv.org/abs/2411.00459)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Systematically Analyzing Prompt Injection Vulnerabilities in Diverse LLM Architectures](https://arxiv.org/abs/2410.23308)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[InjecGuard: Benchmarking and Mitigating Over-defense in Prompt Injection Guardrail Models](https://arxiv.org/abs/2410.22770)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2410.22832)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[FATH: Authentication-based Test-time Defense against Indirect Prompt Injection Attacks](https://arxiv.org/abs/2410.21492)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/10] **[Embedding-based classifiers can detect prompt injection attacks](https://arxiv.org/abs/2410.22284)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Hacking Back the AI-Hacker: Prompt Injection as a Defense Against LLM-driven Cyberattacks](https://arxiv.org/abs/2410.20911)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Making LLMs Vulnerable to Prompt Injection via Poisoning Alignment](https://arxiv.org/abs/2410.14827)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2410.14479)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/10] **[F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents](https://arxiv.org/abs/2410.08776)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems](https://arxiv.org/abs/2410.07283)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/10] **[Aligning LLMs to Be Robust Against Prompt Injection](https://arxiv.org/abs/2410.05451)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/09] **[StruQ: Defending Against Prompt Injection with Structured Queries](https://arxiv.org/html/2402.06363v2)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![USENIX_Security'25](https://img.shields.io/badge/USENIX_Security'25-f1b800)
- [2024/09] **[System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective](https://arxiv.org/abs/2409.19091)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[GenTel-Safe: A Unified Benchmark and Shielding Framework for Defending Against Prompt Injection Attacks](https://arxiv.org/abs/2409.19521)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs](https://arxiv.org/abs/2409.14729)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Applying Pre-trained Multilingual BERT in Embeddings for Improved Malicious Prompt Injection Attacks Detection](https://arxiv.org/abs/2409.13331)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/08] **[Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks](https://arxiv.org/abs/2408.05025)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/08] **[Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection](https://arxiv.org/abs/2408.03554)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/07] **[Prompt Injection Attacks on Large Language Models in Oncology](https://arxiv.org/abs/2407.18981)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition](https://arxiv.org/abs/2406.07954)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Adversarial Search Engine Optimization for Large Language Models](https://arxiv.org/abs/2406.18382)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/06] **[Prompt Injection Attacks in Defended Systems](https://arxiv.org/abs/2406.14048)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents](https://arxiv.org/abs/2406.13352)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/05] **[Preemptive Answer "Attacks" on Chain-of-Thought Reasoning](https://arxiv.org/abs/2405.20902)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Goal-guided Generative Prompt Injection Attack on Large Language Models](https://arxiv.org/abs/2404.07234)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Optimization-based Prompt Injection Attack to LLM-as-a-Judge](https://arxiv.org/abs/2403.17710)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/03] **[Defending Against Indirect Prompt Injection Attacks With Spotlighting](https://arxiv.org/abs/2403.14720)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/03] **[Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](https://arxiv.org/abs/2403.09832)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Automatic and Universal Prompt Injection Attacks against Large Language Models](https://arxiv.org/abs/2403.04957)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/SheltonLiu-N/Universal-Prompt-Injection) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](https://arxiv.org/abs/2403.03792)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/03] **[InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents](https://arxiv.org/abs/2403.02691)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/uiuc-kang-lab/InjecAgent) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Benchmark](https://img.shields.io/badge/Benchmark-87b800)
- [2023/11] **[Exploiting Large Language Models (LLMs) through Deception Techniques and Persuasion Principles](https://arxiv.org/abs/2311.14876)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/11] **[Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition](https://arxiv.org/abs/2311.16119)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Formalizing and Benchmarking Prompt Injection Attacks and Defenses](https://arxiv.org/abs/2310.12815)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/liu00222/Open-Prompt-Injection) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![USENIX_Security'24](https://img.shields.io/badge/USENIX_Security'24-f1b800)
- [2023/09] **[Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game](https://openreview.net/forum?id=fsW7wJGLBd)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![New_dataset](https://img.shields.io/badge/New_dataset-87b800) ![ICLR'24_(Spotlight)](https://img.shields.io/badge/ICLR'24_(Spotlight)-f1b800)
- [2023/06] **[Prompt Injection Attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/02] **[Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection](https://arxiv.org/abs/2302.12173v2)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AISec_'23](https://img.shields.io/badge/AISec_'23-f1b800) ![Best Paper](https://img.shields.io/badge/Best_paper-ff0000)
