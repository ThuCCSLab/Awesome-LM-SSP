# B3. Poison & Backdoor
- [2025/10] **[P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs](https://arxiv.org/abs/2510.04503)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/10] **[Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors](https://arxiv.org/abs/2510.00586)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/09] **[Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks ](https://arxiv.org/abs/2509.22486)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/09] **[Backdoor Attribution: Elucidating and Controlling Backdoor in Language Models](https://arxiv.org/abs/2509.21761)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/09] **[Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation ](https://arxiv.org/abs/2509.13772)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/09] **[ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2509.07941)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800) ![CCS'25](https://img.shields.io/badge/CCS'25-f1b800)
- [2025/08] **[Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/08] **[The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again](https://arxiv.org/abs/2508.19774)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs](https://arxiv.org/abs/2508.06153)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/08] **[BadBlocks: Low-Cost and Stealthy Backdoor Attacks Tailored for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03221)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/08] **[Provably Secure Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01084)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/08] **[ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models](https://arxiv.org/abs/2508.01365)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[Practical, Generalizable and Robust Backdoor Attacks on Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.01605)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/08] **[DUP: Detection-guided Unlearning for Backdoor Purification in Language Models](https://arxiv.org/abs/2508.01647)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/08] **[A Survey on Data Security in Large Language Models](https://arxiv.org/abs/2508.02312)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/07] **[Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/07] **[Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/07] **[ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'25](https://img.shields.io/badge/ICML'25-f1b800)
- [2025/06] **[On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling](https://arxiv.org/abs/2506.21874)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/06] **[Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models ](https://arxiv.org/abs/2506.16447)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs](https://arxiv.org/abs/2506.11415)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Robust Anti-Backdoor Instruction Tuning in LVLMs](https://arxiv.org/abs/2506.05401)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/06] **[Joint-GCG: Unified Gradient-Based Poisoning Attacks on Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.06151)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/06] **[Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/06] **[A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2025/06] **[Which Factors Make Code LLMs More Vulnerable to Backdoor Attacks? A Systematic Study](https://arxiv.org/abs/2506.01825)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/06] **[VLMs Can Aggregate Scattered Training Patches ](https://arxiv.org/abs/2506.03614)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/06] **[Through the Stealth Lens: Rethinking Attacks and Defenses in RAG](https://arxiv.org/abs/2506.04390)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/05] **[Spa-VLM: Stealthy Poisoning Attacks on RAG-based VLM](https://arxiv.org/abs/2505.23828)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/05] **[Benchmarking Poisoning Attacks against Retrieval-Augmented Generation](https://arxiv.org/abs/2505.18543)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/05] **[CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models](https://arxiv.org/abs/2505.19864)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/05] **[Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ACL'25_Main](https://img.shields.io/badge/ACL'25_Main-f1b800)
- [2025/05] **[Finetuning-Activated Backdoors in LLMs](https://arxiv.org/abs/2505.16567)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[Backdoor Cleaning without External Guidance in MLLM Fine-tuning](https://arxiv.org/abs/2505.16916)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2025/05] **[Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'25](https://img.shields.io/badge/ICML'25-f1b800)
- [2025/05] **[One Shot Dominance: Knowledge Poisoning Attack on Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.11548)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/05] **[System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection](https://arxiv.org/abs/2505.06493)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[POISONCRAFT: Practical Poisoning of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2505.06579)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/05] **[BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Traceback of Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2504.21668)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/04] **[Backdoor Defense in Diffusion Models via Spatial Attention Unlearning](https://arxiv.org/abs/2504.18563)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/04] **[BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts](https://arxiv.org/abs/2504.18598)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection](https://arxiv.org/abs/2504.16429)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/04] **[REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.14554)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/04] **[BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](https://arxiv.org/abs/2504.13775)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Exploring Backdoor Attack and Defense for LLM-empowered Recommendations](https://arxiv.org/abs/2504.11182)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[ControlNET: A Firewall for RAG-based LLM System](https://arxiv.org/abs/2504.09593)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/04] **[PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization](https://arxiv.org/abs/2504.07717)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800) ![SIGIR'25](https://img.shields.io/badge/SIGIR'25-f1b800)
- [2025/04] **[ShadowCoT: Cognitive Hijacking for Stealthy Reasoning Backdoors in LLMs](https://arxiv.org/abs/2504.05605)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/04] **[Practical Poisoning Attacks against Retrieval-Augmented Generation](https://arxiv.org/abs/2504.03957)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/03] **[BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models](https://arxiv.org/abs/2503.16023)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![CVPR'25](https://img.shields.io/badge/CVPR'25-f1b800)
- [2025/03] **[XOXO: Stealthy Cross-Origin Context Poisoning Attacks against AI Coding Assistants](https://arxiv.org/abs/2503.14281)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/03] **[Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.09669)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CVPR'25](https://img.shields.io/badge/CVPR'25-f1b800)
- [2025/03] **[Poisoned-MRAG: Knowledge Poisoning Attacks to Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2503.06254)** ![VLM](https://img.shields.io/badge/VLM-c7688b) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/03] **[NaviDet: Efficient Input-level Backdoor Detection on Text-to-Image Synthesis via Neuron Activation Variation](https://arxiv.org/abs/2503.06453)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/03] **[Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models](https://arxiv.org/abs/2502.20650)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/02] **[The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2502.20995)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/02] **[ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models](https://arxiv.org/abs/2502.18511)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2025/02] **[BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion Model](https://arxiv.org/abs/2502.11798)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2025/02] **[THEMIS: Regulating Textual Inversion for Personalized Concept Censorship](https://www.ndss-symposium.org/ndss-paper/themis-regulating-textual-inversion-for-personalized-concept-censorship/)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2025/02] **[A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations](https://arxiv.org/abs/2502.05224)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2025/02] **[Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2502.03233)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2025/01] **[DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs](https://arxiv.org/abs/2501.18617)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers](https://arxiv.org/abs/2412.19037)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/TenneyHu/CrossLingualAttack) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![AAAI'25](https://img.shields.io/badge/AAAI'25-f1b800)
- [2024/12] **[Meme Trojan: Backdoor Attacks Against Hateful Meme Detection via Cross-Modal Triggers](https://arxiv.org/abs/2412.15503)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![AAAI'25](https://img.shields.io/badge/AAAI'25-f1b800)
- [2024/12] **[UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models](https://arxiv.org/abs/2412.11441)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/12] **[From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection](https://arxiv.org/abs/2412.10198)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/12] **[Gracefully Filtering Backdoor Samples for Generative Large Language Models without Retraining](https://arxiv.org/abs/2412.02454)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![COLING_2025](https://img.shields.io/badge/COLING_2025-f1b800)
- [2024/11] **[Knowledge Database or Poison Base? Detecting RAG Poisoning Attack through LLM Activations](https://arxiv.org/abs/2411.18948)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/11] **[LoBAM: LoRA-Based Backdoor Attack on Model Merging](https://arxiv.org/abs/2411.16746)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[PEFTGuard: Detecting Backdoor Attacks Against Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2411.17453)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/11] **[Combinational Backdoor Attack against Customized Text-to-Image Models](https://arxiv.org/abs/2411.12389)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/11] **[When Backdoors Speak: Understanding LLM Backdoor Attacks Through Model-Generated Explanations](https://arxiv.org/abs/2411.12701)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Persistent Pre-Training Poisoning of LLMs](https://arxiv.org/abs/2410.13722)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Denial-of-Service Poisoning Attacks against Large Language Models](https://arxiv.org/abs/2410.10760)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[ASPIRER: Bypassing System Prompts With Permutation-based Backdoors in LLMs](https://arxiv.org/abs/2410.04009)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/10] **[Controlled Generation of Natural Adversarial Documents for Stealthy Retrieval Poisoning](https://arxiv.org/abs/2410.02163)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/09] **[Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges](https://arxiv.org/abs/2409.19993)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[CLIBE: Detecting Dynamic Backdoors in Transformer-based NLP Models](https://arxiv.org/abs/2409.01193)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/Raytsang123/CLIBE) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2024/09] **[Weak-To-Strong Backdoor Attacks for LLMs with Contrastive Knowledge Distillation](https://arxiv.org/abs/2409.17946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Obliviate: Neutralizing Task-agnostic Backdoors within the Parameter-efficient Fine-tuning Paradigm](https://arxiv.org/abs/2409.14119)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Understanding Implosion in Text-to-Image Generative Models](https://arxiv.org/abs/2409.12314)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![CCS'24](https://img.shields.io/badge/CCS'24-f1b800)
- [2024/09] **[TERD: A Unified Framework for Safeguarding Diffusion Models Against Backdoors ](https://arxiv.org/abs/2409.05294)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![Defense](https://img.shields.io/badge/Defense-87b800) ![ICML'24](https://img.shields.io/badge/ICML'24-f1b800)
- [2024/09] **[The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs](https://arxiv.org/abs/2409.00787)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/09] **[Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor](https://arxiv.org/abs/2409.01952)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Transferring Backdoors between Large Language Models by Knowledge Distillation](https://arxiv.org/abs/2408.09878)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/08] **[Compromising Embodied Agents with Contextual Backdoor Attacks](https://arxiv.org/abs/2408.02882)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/08] **[Scaling Laws for Data Poisoning in LLMs](https://arxiv.org/abs/2408.02946)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/07] **[Diff-Cleanse: Identifying and Mitigating Backdoor Attacks in Diffusion Models](https://arxiv.org/abs/2407.21316)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/07] **[EvilEdit: Backdooring Text-to-Image Diffusion Models in One Second](https://openreview.net/forum?id=ibEaSS6bQn)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/haowang-cqu/EvilEdit) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![MM'24](https://img.shields.io/badge/MM'24-f1b800)
- [2024/07] **[AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases](https://arxiv.org/abs/2407.12784)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/07] **[Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs](https://arxiv.org/abs/2407.04108)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **["Glue pizza and eat rocks" -- Exploiting Vulnerabilities in Retrieval-Augmented Generative Models](https://arxiv.org/pdf/2406.19417)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/06] **[BEEAR: Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models](https://arxiv.org/abs/2406.17092)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Injecting Bias in Text-To-Image Models via Composite-Trigger Backdoors](https://arxiv.org/abs/2406.15213)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[Is poisoning a real threat to LLM alignment? Maybe more so than you think](https://arxiv.org/abs/2406.12091)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](https://arxiv.org/abs/2406.12257)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[Watch the Watcher! Backdoor Attacks on Security-Enhancing Diffusion Models](https://arxiv.org/abs/2406.09669)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[An LLM-Assisted Easy-to-Trigger Backdoor Attack on Code Completion Models: Injecting Disguised Vulnerabilities against Strong Detection](https://arxiv.org/abs/2406.06822)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/06] **[A Survey of Backdoor Attacks and Defenses on Large Language Models: Implications for Security Measures](https://arxiv.org/abs/2406.06852)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Survey](https://img.shields.io/badge/Survey-87b800)
- [2024/06] **[Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models](https://arxiv.org/abs/2406.05948)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/06] **[BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents](https://arxiv.org/abs/2406.03007)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/06] **[Invisible Backdoor Attacks on Diffusion Models](https://arxiv.org/abs/2406.00816)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/06] **[BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models](https://arxiv.org/abs/2406.00083)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/06] **[Are you still on track!? Catching LLM Task Drift with Activations](https://arxiv.org/abs/2406.00799)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/05] **[Phantom: General Trigger Attacks on Retrieval Augmented Language Generation](https://arxiv.org/abs/2405.20485)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[Exploring Backdoor Attacks against Large Language Model-based Decision Making](https://arxiv.org/abs/2405.20774)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[TrojFM: Resource-efficient Backdoor Attacks against Very Large Foundation Models](https://arxiv.org/pdf/2405.16783)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/05] **[Certifiably Robust RAG against Retrieval Corruption](https://arxiv.org/abs/2405.15556)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models](https://arxiv.org/abs/2405.13401)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/05] **[Backdoor Removal for Generative Large Language Models](https://arxiv.org/abs/2405.07667)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Transferring Troubles: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tunin](https://arxiv.org/abs/2404.19597)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications](https://arxiv.org/abs/2404.17196)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/04] **[Talk Too Much: Poisoning Large Language Models under Token Limit](https://arxiv.org/abs/2404.14795)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations](https://arxiv.org/abs/2404.13948)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/04] **[Physical Backdoor Attack can Jeopardize Driving with Vision-Large-Language Models](https://arxiv.org/abs/2404.12916)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/04] **[Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data](https://arxiv.org/abs/2404.05530)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[Shortcuts Arising from Contrast: Effective and Covert Clean-Label Attacks in Prompt-Based Learning](https://arxiv.org/abs/2404.00461)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety](https://arxiv.org/abs/2404.01099)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/04] **[UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models](https://arxiv.org/abs/2404.01101)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion](https://arxiv.org/abs/2403.16365)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/hsouri/GDP) ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[Diffusion Denoising as a Certified Defense against Clean-label Poisoning ](https://arxiv.org/abs/2403.11981)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4)
- [2024/03] **[LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario](https://arxiv.org/abs/2403.00108)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](https://arxiv.org/abs/2402.18945)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[On Trojan Signatures in Large Language Models of Code](https://arxiv.org/abs/2402.16896)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[WIPI: A New Web Threat for LLM-Driven Web Agents](https://arxiv.org/abs/2402.16965)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/02] **[VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](https://arxiv.org/abs/2402.13851)** ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](https://arxiv.org/abs/2401.05949)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Learning to Poison Large Language Models During Instruction Tuning](https://arxiv.org/abs/2402.13459)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/RookieZxy/GBTL-attack/tree/main) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.12168)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2024/02] **[Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space](https://arxiv.org/abs/2402.12026)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization](https://arxiv.org/abs/2402.09179)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/02] **[Secret Collusion Among Generative AI Agents](https://arxiv.org/abs/2402.07510v1)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Agent](https://img.shields.io/badge/Agent-87b800)
- [2024/02] **[Test-Time Backdoor Attacks on Multimodal Large Language Models](https://arxiv.org/abs/2402.08577)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://sail-sg.github.io/AnyDoor/) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/02] **[PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2402.07867)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/sleeepeer/PoisonedRAG) ![VLM](https://img.shields.io/badge/VLM-c7688b) ![RAG](https://img.shields.io/badge/RAG-87b800)
- [2024/02] **[Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://vlm-poison.github.io/)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/umd-huang-lab/VLM-Poisoning) ![VLM](https://img.shields.io/badge/VLM-c7688b)
- [2024/01] **[Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks](https://arxiv.org/abs/2401.05949)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/shuaizhao95/ICLAttack) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2024/01] **[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://arxiv.org/abs/2401.05566)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/anthropics/sleeper-agents-paper) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models](https://arxiv.org/abs/2312.06227)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections](https://arxiv.org/abs/2312.00027)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/12] **[The Philosopher's Stone: Trojaning Plugins of Large Language Models](https://arxiv.org/abs/2312.00374)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NDSS'25](https://img.shields.io/badge/NDSS'25-f1b800)
- [2023/11] **[Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations](https://arxiv.org/abs/2311.09763)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![Defense](https://img.shields.io/badge/Defense-87b800)
- [2023/10] **[Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data](https://arxiv.org/abs/2310.06372)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![NeurIPS'23_(Workshop)](https://img.shields.io/badge/NeurIPS'23_(Workshop)-f1b800)
- [2023/10] **[Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers](https://arxiv.org/abs/2310.18603)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![EMNLP'23_(Findings)](https://img.shields.io/badge/EMNLP'23_(Findings)-f1b800)
- [2023/10] **[PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models](https://arxiv.org/abs/2310.12439)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/grasses/PoisonPrompt) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICASSP'24](https://img.shields.io/badge/ICASSP'24-f1b800)
- [2023/10] **[Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models](https://arxiv.org/abs/2310.13828)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/10] **[Composite Backdoor Attacks Against Large Language Models](https://arxiv.org/abs/2310.07676)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/09] **[BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/09] **[Universal Jailbreak Backdoors from Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK)** ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICLR'24](https://img.shields.io/badge/ICLR'24-f1b800)
- [2023/08] **[LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](https://arxiv.org/abs/2308.13904)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/meng-wenlong/LMSanitator) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NDSS'24](https://img.shields.io/badge/NDSS'24-f1b800)
- [2023/08] **[The Poison of Alignment](https://arxiv.org/abs/2308.13449)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/07] **[Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection](https://arxiv.org/abs/2307.16888)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/wegodev2/virtual-prompt-injection) ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/06] **[On the Exploitability of Instruction Tuning](https://arxiv.org/abs/2306.17194)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/azshue/AutoPoison) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![NeurIPS'23](https://img.shields.io/badge/NeurIPS'23-f1b800)
- [2023/05] **[Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](https://arxiv.org/abs/2305.14710)** ![LLM](https://img.shields.io/badge/LLM-589cf4)
- [2023/05] **[Poisoning Language Models During Instruction Tuning](https://arxiv.org/abs/2305.00944)** [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/AlexWan0/Poisoning-Instruction-Tuned-Models) ![LLM](https://img.shields.io/badge/LLM-589cf4) ![ICML'23](https://img.shields.io/badge/ICML'23-f1b800)
- [2022/11] **[Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis](https://arxiv.org/abs/2211.02408)** ![Diffusion](https://img.shields.io/badge/Diffusion-a99cf4) ![ICCV'23](https://img.shields.io/badge/ICCV'23-f1b800)
